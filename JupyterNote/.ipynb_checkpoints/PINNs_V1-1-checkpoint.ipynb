{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Hands-on Introduction to Physics-Informed Neural Networks\n",
    "\n",
    "\n",
    "## Authors\n",
    "+ Atharva Hans\n",
    "     + Graduate Research Assistant \n",
    "     + [Predictive Science Lab](https://www.predictivesciencelab.org/)\n",
    "     + School of Mechanical Engineering, Purdue University\n",
    "     + Email: hans1@purdue.edu\n",
    "     \n",
    "     \n",
    "+ Ilias Bilionis\n",
    "     + Associate Professor\n",
    "     + [Predictive Science Lab](https://www.predictivesciencelab.org/)\n",
    "     + School of Mechanical Engineering, Purdue University\n",
    "     + Email: ibilion@purdue.edu\n",
    "\n",
    "\n",
    "## Objectives\n",
    "\n",
    "\n",
    "+ Learn how to solve ODEs with neural networks.\n",
    "+ Learn how we can use neural networks to obtain simulator free solution for forward model evaluations; using a simple example from solid mechanics.\n",
    "+ See how network structure affects convergence.\n",
    "+ Introduce the problem of spectral bias in DNN's and see how we can solve it.\n",
    "\n",
    "## References\n",
    "\n",
    "+ [Artificial Neural Networks for Solving Ordinary and Partial Differential Equations](https://arxiv.org/pdf/physics/9705023.pdf)\n",
    "+ [Deep Residual Learning for Image Recognition](https://arxiv.org/pdf/1512.03385.pdf)\n",
    "+ [On the Spectral Bias of Neural Networks](http://proceedings.mlr.press/v97/rahaman19a/rahaman19a.pdf)\n",
    "+ [Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains](https://arxiv.org/pdf/2006.10739.pdf)\n",
    "\n",
    "\n",
    "<b>If for some reason this notebook doesn't work, or is slow, you can run the notebook [here](https://colab.research.google.com/drive/1Dmbh--_YHajQgwZzXr-iYCGbMHNblB_s?usp=sharing). All you need is a google account.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "## let's import the relevant libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from time import perf_counter\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import requests\n",
    "import os\n",
    "\n",
    "## check if GPU is available and use it; otherwise use CPU\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Single ODE\n",
    "\n",
    "Consider the ode:\n",
    "$$\n",
    "\\frac{d\\Psi}{dx} = f(x, \\Psi),\n",
    "$$\n",
    "with $x \\in [0,1]$ and initial conditions (IC):\n",
    "$$\n",
    "\\Psi(0) = A.\n",
    "$$\n",
    "We write the trial solution by:\n",
    "$$\n",
    "\\hat{\\Psi}(x; \\theta) = A + x N(x; \\theta),\n",
    "$$\n",
    "where $N(x; \\theta)$ is a neural network (NN).\n",
    "The solution is $\\hat{\\Psi}(x;\\theta)$ automatically satisfied the initial conditions.\n",
    "The loss function we would like to minimize to train the NN is:\n",
    "$$\n",
    "L(\\theta) = \\int_0^1 \\left[\\frac{d\\hat{\\Psi}(x;\\theta)}{dx} - f(x,\\hat{\\Psi}(x;\\theta))\\right]^2dx\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N is a Neural Network - This is exactly the network used by Lagaris et al. 1997\n",
    "N = nn.Sequential(nn.Linear(1, 50), nn.Sigmoid(), nn.Linear(50,1, bias=False))\n",
    "\n",
    "# Initial condition\n",
    "A = 0.\n",
    "\n",
    "# The Psi_t function\n",
    "Psi_t = lambda x: A + x * N(x)\n",
    "\n",
    "# The right hand side function\n",
    "f = lambda x, Psi: torch.exp(-x / 5.0) * torch.cos(x) - Psi / 5.0\n",
    "\n",
    "# The loss function\n",
    "def loss(x):\n",
    "    x.requires_grad = True\n",
    "    outputs = Psi_t(x)\n",
    "    Psi_t_x = torch.autograd.grad(outputs, x, grad_outputs=torch.ones_like(outputs),\n",
    "                                  create_graph=True)[0]\n",
    "    return torch.mean((Psi_t_x - f(x, outputs)) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will use the method that we find in [Lagaris et al](https://arxiv.org/pdf/physics/9705023.pdf).\n",
    "Instead of using stochastic optimization, they use a lot of points to estimate the loss integral (We will use 100) and then they just do gradient-based optimization (We will do BFGS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize (same algorithm as in Lagaris)\n",
    "optimizer = torch.optim.LBFGS(N.parameters())\n",
    "\n",
    "# The collocation points used by Lagaris\n",
    "x = torch.Tensor(np.linspace(0, 2, 100)[:, None])\n",
    "\n",
    "# Run the optimizer\n",
    "def closure():\n",
    "    optimizer.zero_grad()\n",
    "    l = loss(x)\n",
    "    l.backward()\n",
    "    return l\n",
    "    \n",
    "for i in range(10):\n",
    "    optimizer.step(closure)\n",
    "\n",
    "# Let's compare the result to the true solution\n",
    "xx = np.linspace(0, 2, 100)[:, None]\n",
    "with torch.no_grad():\n",
    "    yy = Psi_t(torch.Tensor(xx)).numpy()\n",
    "yt = np.exp(-xx / 5.0) * np.sin(xx)\n",
    "\n",
    "fig, ax = plt.subplots(dpi=100)\n",
    "ax.plot(xx, yt, label='True')\n",
    "ax.plot(xx, yy, '--', label='Neural network approximation')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$\\Psi(x)$')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use stochastic gradient descent to minimize the loss integral."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to reinitialize the network\n",
    "N = nn.Sequential(nn.Linear(1, 50), nn.Sigmoid(), nn.Linear(50,1, bias=False))\n",
    "\n",
    "# Let's see now if a stochastic optimizer makes a difference\n",
    "adam = torch.optim.Adam(N.parameters(), lr=0.01)\n",
    "\n",
    "# The batch size you want to use (how many points to use per iteration)\n",
    "n_batch = 5\n",
    "\n",
    "# The maximum number of iterations to do\n",
    "max_it = 1000\n",
    "\n",
    "for i in range(max_it):\n",
    "    # Randomly pick n_batch random x's:\n",
    "    x = 2 * torch.rand(n_batch, 1)\n",
    "    # Zero-out the gradient buffers\n",
    "    adam.zero_grad()\n",
    "    # Evaluate the loss\n",
    "    l = loss(x)\n",
    "    # Calculate the gradients\n",
    "    l.backward()\n",
    "    # Update the network\n",
    "    adam.step()\n",
    "    # Print the iteration number\n",
    "    if i % 100 == 99:\n",
    "        print(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compare the result to the true solution\n",
    "xx = np.linspace(0, 2, 100)[:, None]\n",
    "with torch.no_grad():\n",
    "    yy = Psi_t(torch.Tensor(xx)).numpy()\n",
    "yt = np.exp(-xx / 5.0) * np.sin(xx)\n",
    "\n",
    "fig, ax = plt.subplots(dpi=100)\n",
    "ax.plot(xx, yt, label='True')\n",
    "ax.plot(xx, yy, '--', label='Neural network approximation')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$\\Psi(x)$')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "+ Change `n_batch` to just 1. Does the algorithm work? Did you have to increase the iterations to achieve the same accuracy?\n",
    "+ Modify the code, so that you now solve the problem for $x$ between 0 and 5. Play with the `n_batch` and `max_it` until you get a good solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Deformation of a compressible neo-Hookean material\n",
    "\n",
    "+ Consider a neo-Hookean square body defined on $(x,y) \\in [0,1]^2$. Let $u(x,y)$ describe the displacement field for this body. This body is subjected to the following displacement boundary conditions:\n",
    "$$\n",
    "u_x(0,y) = 0,\n",
    "$$\n",
    "$$\n",
    "u_y(0,y) = 0,\n",
    "$$\n",
    "$$\n",
    "u_x(1,y) = \\delta,\n",
    "$$\n",
    "$$\n",
    "u_y(1,y) = 0,\n",
    "$$\n",
    "with $\\delta$ referring to the applied displacement along the x-direction.\n",
    "\n",
    "\n",
    "+ For this hyperelastic material, the stored energy $E_b$ in the body can be expressed in as:\n",
    "$$\n",
    "E_b(u(x,y)) = \\int_{[0,1]^2}\\left\\{\\frac{1}{2}(\\sum_{i}\\sum_{j}{F_{ij}^2} - 2)- \\ln(\\det(F)) + 50\\ln(\\det(F))^2\\right\\} dxdy,\n",
    "$$\n",
    "with,\n",
    "$$\n",
    "F = I + \\nabla u,\n",
    "$$\n",
    "where $I$ is an identity matrix.\n",
    "\n",
    "\n",
    "+ The final orientation of this body is described by a displacement field that minimizes the stored energy $E_b$.\n",
    "\n",
    "\n",
    "+ Let's describe the horizontal and the vertical components of displacement field ($u_x$ and $u_y$ respectively) for this body as:\n",
    "$$\n",
    "u_x(x,y) = \\delta - \\delta(1-x) + x(1-x)N_x(x,y;\\theta),\n",
    "$$\n",
    "and,\n",
    "$$\n",
    "u_y(x,y) = x(1-x)N_y(x,y;\\theta)\n",
    "$$\n",
    "where $N_x(x,y;\\theta)$ and $N_y(x,y;\\theta)$ are neural networks. Notice $u_x(x,y)$ and $u_y(x,y)$ automatically satisfies the boundary conditions. \n",
    "\n",
    "\n",
    "+ The loss function that we would like to minimize to train the NN is:\n",
    "$$\n",
    "L(\\theta) = E_b(u(x,y))\n",
    "$$\n",
    "We will Monte Carlo for estimating the integral.\n",
    "\n",
    "Next, let's define a few helper functions which will be used throughout this activity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gradient Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(outputs, inputs):\n",
    "    \"\"\"\n",
    "    This is useful for taking derivatives\n",
    "    \"\"\"\n",
    "    return torch.autograd.grad(outputs, inputs, grad_outputs=torch.ones_like(outputs), create_graph=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Displacement Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def u_r(x, delta, network1, network2, scale_network_out):\n",
    "    \"\"\"\n",
    "    This function returns displacement field u_r.\n",
    "    u_r comprises the horizontal and the vertical components \n",
    "    of displacement (u_x and u_y respectively.)\n",
    "    -> x should contain the X and Y coordinates. Should be of shape: (batch size x 2)\n",
    "    -> delta is the Direchlet boundary condition. Should be something similar to: torch.tensor(([right_edge_displacement, 0.]))\n",
    "    \"\"\"\n",
    "    u_x = delta[0] - delta[0]*(1. - x[:,0][:,None]) + x[:,0][:,None]*(1. - x[:,0][:,None])*network1(x)/scale_network_out\n",
    "    u_y = x[:,0][:,None]*(1. - x[:,0][:,None])*network2(x)/scale_network_out\n",
    "    u_r = torch.cat((u_x, u_y), dim=1)\n",
    "    return u_r "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(X, u_r):\n",
    "    \"\"\"\n",
    "    This is our loss function that we want to minimize.\n",
    "    -> X: inputs data. Should be of shape: (batch_size x 2)\n",
    "    -> Displacement function (which requires only X as input). \n",
    "    \"\"\"\n",
    "    X.requires_grad = True\n",
    "    u_r = u_r(X)\n",
    "    g_u_x = grad(u_r[:, 0], X)[:, None, :]\n",
    "    g_u_y = grad(u_r[:, 1], X)[:, None, :]\n",
    "    J = torch.cat([g_u_x, g_u_y], 1)\n",
    "    I = torch.eye(2).repeat(X.shape[0], 1, 1)\n",
    "    F = I + J\n",
    "    log_det_F = torch.log(F[:, 0, 0] * F[:, 1, 1] - F[:, 0, 1] * F[:, 1, 0])\n",
    "    res = (0.5 * (torch.einsum('ijk,ijk->i', F, F) - 2) - log_det_F + 50 * log_det_F ** 2)\n",
    "    return torch.mean(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_MultiFieldFracture_seperate_net(net1, net2, right_edge_displacement, batch_size_X, max_iter, print_results_every, scale_network_out):\n",
    "    \"\"\"\n",
    "    Trains given networks.\n",
    "    -> right_edge_displacement: corresponds to boundary condition (delta) on the right edge. Example: 0.5\n",
    "    -> batch_size_X: how many X's do you want to use for each iteration. Example: 10\n",
    "    -> print_results_every: how often do you want to display the results. Example: 100\n",
    "    \"\"\"\n",
    "    print('\\n\\nStarting training loop with seperate networks for u_x and u_y...\\n\\n')\n",
    "    print('Right Edge Displacement: %.3f'%right_edge_displacement, '\\t\\tbatch_size_X: %d'%batch_size_X, '\\t\\tmax_iter: %d\\n'%max_iter)\n",
    "    network1 = net1\n",
    "    network1 = network1.to(device)\n",
    "    network2 = net2\n",
    "    network2 = network2.to(device)\n",
    "    \n",
    "    # Direchlet boundary condition\n",
    "    delta = torch.tensor(([right_edge_displacement, 0.]), device=device) # this contains the x and the y coordinate of the displacement applied on the body\n",
    "    # Here are the parameters that we would like to optimze\n",
    "    parameters = list(network1.parameters()) + list(network2.parameters())\n",
    "    # Initialize the optimizer - Notice that it needs to know about the\n",
    "    # parameters it it optimizing\n",
    "    optimizer = torch.optim.Adam(parameters, lr=1e-4) # lr is the learning rate\n",
    "    # Records time the loop starts\n",
    "    start_time = perf_counter()\n",
    "    # Some place to hold the training loss for visualizing it later\n",
    "    loss_list = []\n",
    "    elapsed_time = 0.0\n",
    "    running_loss = 0.0\n",
    "    for i in range(max_iter):\n",
    "        X = torch.distributions.Uniform(0, 1).sample((batch_size_X, 2))\n",
    "        X = X.to(device)\n",
    "        # This is essential for the optimizer to keep \n",
    "        # track of the gradients correctly \n",
    "        # It is using some buffers internally that need to \n",
    "        # be manually zeroed on each iteration.\n",
    "        # This is because the optimizer doesn't know when you are done with the\n",
    "        # calculation of the loss.\n",
    "        optimizer.zero_grad()\n",
    "        # Evaluate the loss - That's what you are minimizing\n",
    "        l = loss(X, partial(u_r, delta=delta, network1=network1, network2=network2, scale_network_out=scale_network_out))\n",
    "        # Add the loss to the running loss\n",
    "        running_loss += l.item()\n",
    "        # Evaluate the derivative of the loss with respect to\n",
    "        # all parameters\n",
    "        l.backward()\n",
    "        # And now you are ready to make a step\n",
    "        optimizer.step()\n",
    "        if (i+1)%print_results_every == 0:\n",
    "            # Print loss, time elapsed every \"print_results_every\"# iterations\n",
    "            current_time = perf_counter()\n",
    "            elapsed_time = current_time - start_time \n",
    "            print('[iter: %d]'%(i+1), '\\t\\telapsed_time: %3d secs'%elapsed_time, '\\t\\tLoss: ', running_loss/print_results_every)\n",
    "            loss_list.append(running_loss/print_results_every)\n",
    "            running_loss = 0.0\n",
    "            \n",
    "    return loss_list, network1, network2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_capacity(net):\n",
    "    \"\"\"\n",
    "    Prints the number of parameters and the number of layers in the network\n",
    "    -> Requires a neural network as input\n",
    "    \"\"\"\n",
    "    number_of_learnable_params = sum(p.numel() for p in net.parameters() if p.requires_grad)\n",
    "    num_layers = len(list(net.parameters()))\n",
    "    print(\"\\nThe number of layers in the model: %d\" % num_layers)\n",
    "    print(\"The number of learnable parameters in the model: %d\\n\" % number_of_learnable_params)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(loss, label):\n",
    "    \"\"\"\n",
    "    Plots the loss function.\n",
    "    -> loss: list containing the losses\n",
    "    -> label: label for this loss\n",
    "    \"\"\"\n",
    "    ax.plot(100*np.arange(len(loss)), loss, label='%s'%label)\n",
    "    ax.set_xlabel('Iterations')\n",
    "    ax.set_ylabel('Loss')\n",
    "    plt.legend(loc='best')\n",
    "    \n",
    "    \n",
    "def plot_displacement(network1, network2,\n",
    "                      right_edge_displacement,\n",
    "                      scale_network_out,\n",
    "                      num_samples_X = 200, s = 4,\n",
    "                      ):\n",
    "    \n",
    "    \"\"\"\n",
    "    Plots the horizontal and vertical components of displacements at given number of points.\n",
    "    -> network1: neural network 1\n",
    "    -> network2: neural network 2\n",
    "    -> right_edge_displacement: corresponds to boundary condition (delta) on the right edge. Example: 0.5\n",
    "    -> num_samples_X: number of grid points\n",
    "    -> s: Marker size\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Direchlet Boundary condition\n",
    "    delta = torch.tensor(([right_edge_displacement, 0.])) # this contains the x and the y coordinate of the displacement applied on the body    \n",
    "    \n",
    "    for i in range(1):\n",
    "        figure, ax = plt.subplots(1,2, figsize=(20,5))\n",
    "        \n",
    "        X_init = torch.linspace(0, 1, num_samples_X)\n",
    "        xx, yy = torch.meshgrid(X_init, X_init)\n",
    "        X_init = torch.cat((xx.reshape(-1)[:,None], yy.reshape(-1)[:,None]), axis=1)\n",
    "\n",
    "        displacement = u_r(X_init, delta, network1, network2, scale_network_out).detach().numpy()\n",
    "        X_final_dual = X_init + displacement \n",
    "        X_final_dual = X_final_dual.numpy()\n",
    "    \n",
    "        sc1 = ax[0].scatter(X_final_dual[:,0], X_final_dual[:,1], s=s, c = displacement[:,0], cmap=plt.cm.get_cmap('copper'))\n",
    "        plt.colorbar(sc1, ax=ax[0])\n",
    "        ax[0].set_title('Horiontal Displacement ($u_x$)')\n",
    "    \n",
    "        sc2 = ax[1].scatter(X_final_dual[:,0], X_final_dual[:,1], s=s, c = displacement[:,1], cmap=plt.cm.get_cmap('copper'))\n",
    "        plt.colorbar(sc2, ax=ax[1])\n",
    "        ax[1].set_title('Vertical Displacement ($u_y$)')\n",
    "    \n",
    "        plt.tight_layout()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with displacement ($\\delta$) of 0.5, so\n",
    "$$\n",
    "u_x(1,y) = 0.5.\n",
    "$$\n",
    "\n",
    "<b>Note: Solving this problem for displacement ($\\delta$) of 0.5 in FEM requires us to break down displacement in smaller chunks. For example, we would first solve for the displacement field for $\\delta=0.1$ then using that displacement field as baseline, we would further solve it for $\\delta=0.1$ and so on...(until we reach $\\delta=0.5$).\n",
    "\n",
    "In general, FEM solver fails when we try solving for the displacement ($\\delta$) of 0.5 at once.</b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boundary condition (delta) on the right edge\n",
    "right_edge_displacement = 0.5 # try 0.05\n",
    "# Scale down the network output to ensure we get +ve determinant of the Jacobian.\n",
    "# We have to scale the output so that as the training begins we don't initialize a displacement \n",
    "# which has no physical meaning. \n",
    "# For example, the determinant of the Jacobian cannot be negative \n",
    "# since that would mean negative volume; which has no physical meaning. \n",
    "scale_network_out = 5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A: Simple Fully Connected Neural Network\n",
    "\n",
    "Let's begin with a simplest case where we have simple fully connected neural networks for both the horizontal displacement $u_x$ and the vertical displacement $u_y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the network for the u_x\n",
    "simple_net1 = nn.Sequential(nn.Linear(2,50), \n",
    "                     nn.Sigmoid(), \n",
    "                     nn.Linear(50,50), \n",
    "                     nn.Sigmoid(),\n",
    "                     nn.Linear(50,50), \n",
    "                     nn.Sigmoid(),\n",
    "                     nn.Linear(50,50), \n",
    "                     nn.Sigmoid(),\n",
    "                     nn.Linear(50,50), \n",
    "                     nn.Sigmoid(),\n",
    "                     nn.Linear(50,50), \n",
    "                     nn.Sigmoid(),\n",
    "                     nn.Linear(50,50), \n",
    "                     nn.Sigmoid(),\n",
    "                     nn.Linear(50,1)\n",
    "                    )\n",
    "\n",
    "# here is the network for the u_y\n",
    "simple_net2 = nn.Sequential(nn.Linear(2,50), \n",
    "                     nn.Sigmoid(), \n",
    "                     nn.Linear(50,50), \n",
    "                     nn.Sigmoid(),\n",
    "                     nn.Linear(50,50), \n",
    "                     nn.Sigmoid(),\n",
    "                     nn.Linear(50,50), \n",
    "                     nn.Sigmoid(),\n",
    "                     nn.Linear(50,50), \n",
    "                     nn.Sigmoid(),\n",
    "                     nn.Linear(50,50), \n",
    "                     nn.Sigmoid(),\n",
    "                     nn.Linear(50,50), \n",
    "                     nn.Sigmoid(),\n",
    "                     nn.Linear(50,1)\n",
    "                    )\n",
    "\n",
    "# here is how we can find the number of layers and number of model parameters in each network\n",
    "model_capacity(simple_net1)\n",
    "model_capacity(simple_net2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is just a sanity check that we start from a candidate solution that indeed satisfies the boundary conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_displacement(simple_net1, simple_net2,\n",
    "                  right_edge_displacement, scale_network_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the network for 5000 iterations and batch size of 10. Training should take around 6-8 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss_list_simple_net, simple_net1, simple_net2 = train_MultiFieldFracture_seperate_net(net1=simple_net1, net2=simple_net2, \n",
    "                                                                      right_edge_displacement = right_edge_displacement, \n",
    "                                                                      batch_size_X = 10, \n",
    "                                                                      max_iter = 5000, \n",
    "                                                                      print_results_every = 100, scale_network_out=scale_network_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analyze Training Loss and Material Displacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the training loss\n",
    "figure, ax = plt.subplots(dpi=100)\n",
    "plot_loss(loss_list_simple_net, 'Simple Net')\n",
    "\n",
    "if right_edge_displacement==0.05:\n",
    "    ax.plot(100*np.arange(len(loss_list_simple_net)), 0.006*np.ones(len(loss_list_simple_net)), label='FEM Energy')\n",
    "elif right_edge_displacement==0.5:\n",
    "    ax.plot(100*np.arange(len(loss_list_simple_net)), 0.43*np.ones(len(loss_list_simple_net)), label='FEM Energy')\n",
    "plt.legend(loc='best'); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the body post-training\n",
    "plot_displacement(simple_net1, simple_net2, right_edge_displacement, scale_network_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above loss might or might not converge (to the FEM solution) depending on the number of iterations we trained the network and on the boundary displacement that we choose. If we run it long enough (~20,000 iterations), it might converge. \n",
    "\n",
    "Unlike the first example, where we were trying to solve a simple ODE (and the neural network wasn't very deep), here we have relatively deeper networks which may pose two issues:\n",
    "+ [Vanishing gradients](https://arxiv.org/pdf/1512.03385.pdf)\n",
    "+ [Bias towards lower frequency functions](http://proceedings.mlr.press/v97/rahaman19a/rahaman19a.pdf)\n",
    "\n",
    "Is there anything we can do to speed up the convergence?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: ResNet Architecture \n",
    "\n",
    "In this part let's introduce a ResNet architecture for our neural networks and investigate if we get better convergence. \n",
    "ResNet architecture includes skip-connections between the layers which has been [shown](https://arxiv.org/pdf/1512.03385.pdf) to overcome the vanishing gradient problem (and help with convergence).\n",
    "\n",
    "Figure 1a below shows a schematic of a deep ResNet with a dense input layer, followed by K residual blocks and an output layer. \n",
    "\n",
    "Figure 1b shows s single residual block with L layers. Pay attention to how we have a skip-connection from the input $z^{(i-1,0)}$ to the output $z^{(i,0)}$. The skip connections lead to better backpropagation of the loss and helps in better convergence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# schematic of a deep ResNet\n",
    "Image.open('ResNet.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a `DenseResNet` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseResNet(nn.Module):\n",
    "    \"\"\"\n",
    "    This is a ResNet Class.\n",
    "    -> dim_in: network's input dimension\n",
    "    -> dim_out: network's output dimension\n",
    "    -> num_resnet_blocks: number of ResNet blocks\n",
    "    -> num_layers_per_block: number of layers per ResNet block\n",
    "    -> num_neurons: number of neurons in each layer\n",
    "    -> activation: Non-linear activations function that you want to use. E.g. nn.Sigmoid(), nn.ReLU()\n",
    "    -> fourier_features: whether to pass the inputs through Fourier mapping. E.g. True or False\n",
    "    -> m_freq: how many frequencies do you want the inputs to be mapped to\n",
    "    -> sigma: controls the spectrum of frequencies. \n",
    "              If sigma is greater more frequencies are consider. \n",
    "              You can also look at it as sampling from the standard normal, Z~N(0, 1), \n",
    "              and mapping to another normal, X~N(\\mu, \\sigma^2), using x = mu + sigma*z.\n",
    "    -> tune_beta: do you want to consider the parameter beta in the activation functions in each layer? E.g., Tanh(beta*x).\n",
    "                  In practice it is observed that training beta (i.e. tune_beta=True) could improve convergence. \n",
    "                  If tune_beta=False, you get the a fixed beta i.e. beta=1.\n",
    "    -> The method model_capacity() returns the number of layers and parameters in the network.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_in=2, dim_out=1, num_resnet_blocks=3, \n",
    "                 num_layers_per_block=2, num_neurons=50, activation=nn.Sigmoid(), \n",
    "                 fourier_features=False, m_freqs=100, sigma=10, tune_beta=False):\n",
    "        super(DenseResNet, self).__init__()\n",
    "\n",
    "        self.num_resnet_blocks = num_resnet_blocks\n",
    "        self.num_layers_per_block = num_layers_per_block\n",
    "        self.fourier_features = fourier_features\n",
    "        self.activation = activation\n",
    "        self.tune_beta = tune_beta\n",
    "\n",
    "        if tune_beta:\n",
    "            self.beta0 = nn.Parameter(torch.ones(1, 1))\n",
    "            self.beta = nn.Parameter(torch.ones(self.num_resnet_blocks, self.num_layers_per_block))\n",
    "\n",
    "        else: \n",
    "            self.beta0 = torch.ones(1, 1)\n",
    "            self.beta = torch.ones(self.num_resnet_blocks, self.num_layers_per_block)\n",
    "\n",
    "        self.first = nn.Linear(dim_in, num_neurons)\n",
    "\n",
    "        self.resblocks = nn.ModuleList([\n",
    "            nn.ModuleList([nn.Linear(num_neurons, num_neurons) \n",
    "                for _ in range(num_layers_per_block)]) \n",
    "            for _ in range(num_resnet_blocks)])\n",
    "\n",
    "        self.last = nn.Linear(num_neurons, dim_out)\n",
    "\n",
    "        if fourier_features:\n",
    "            self.first = nn.Linear(2*m_freqs, num_neurons)\n",
    "            self.B = nn.Parameter(sigma*torch.randn(dim_in, m_freqs)) # to converts inputs to m_freqs\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.fourier_features:\n",
    "            cosx = torch.cos(torch.matmul(x, self.B))\n",
    "            sinx = torch.sin(torch.matmul(x, self.B))\n",
    "            x = torch.cat((cosx, sinx), dim=1)\n",
    "            x = self.activation(self.beta0*self.first(x)) \n",
    "\n",
    "        else:\n",
    "            x = self.activation(self.beta0*self.first(x))\n",
    "\n",
    "        for i in range(self.num_resnet_blocks):\n",
    "            z = self.activation(self.beta[i][0]*self.resblocks[i][0](x))\n",
    "\n",
    "            for j in range(1, self.num_layers_per_block):\n",
    "                z = self.activation(self.beta[i][j]*self.resblocks[i][j](z))\n",
    "\n",
    "            x = z + x\n",
    "\n",
    "        out = self.last(x)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def model_capacity(self):\n",
    "        \"\"\"\n",
    "        Prints the number of parameters and the number of layers in the network\n",
    "        \"\"\"\n",
    "        number_of_learnable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        num_layers = len(list(self.parameters()))\n",
    "        print(\"\\n\\nThe number of layers in the model: %d\" % num_layers)\n",
    "        print(\"\\nThe number of learnable parameters in the model: %d\" % number_of_learnable_params)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_net1 = DenseResNet(dim_in=2, dim_out=1, num_resnet_blocks=3, \n",
    "                        num_layers_per_block=2, num_neurons=50, activation=nn.Sigmoid(), \n",
    "                        fourier_features=False, m_freqs=100, sigma=10, tune_beta=False)\n",
    "\n",
    "dense_net2 = DenseResNet(dim_in=2, dim_out=1, num_resnet_blocks=3, \n",
    "                        num_layers_per_block=2, num_neurons=50, activation=nn.Sigmoid(), \n",
    "                        fourier_features=False, m_freqs=100, sigma=10, tune_beta=False)\n",
    "\n",
    "# Here is the model capacity:\n",
    "model_capacity(dense_net1)\n",
    "model_capacity(dense_net2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Pay attention: We have the same number of parameters as in Part A</b>. Only difference now is that the network has a ResNet architecture (allows better back propagation of loss). Let's train this model and see if this helps in convergence.\n",
    "\n",
    "Here is just a sanity check that we start from a candidate solution that indeed satisfies the boundary conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the body pre-training and make sure the boundary conditions are satified\n",
    "plot_displacement(dense_net1, dense_net2, right_edge_displacement, scale_network_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the network for 5000 iterations and batch size of 10. Training should take around 6-8 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss_list_dense, dense_net1, dense_net2 = train_MultiFieldFracture_seperate_net(net1=dense_net1, net2=dense_net2, \n",
    "                                                                      right_edge_displacement = right_edge_displacement, \n",
    "                                                                      batch_size_X = 10, \n",
    "                                                                      max_iter = 5000, \n",
    "                                                                      print_results_every = 100, scale_network_out=scale_network_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analyze Training Loss and Material Displacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the training loss\n",
    "figure, ax = plt.subplots(dpi=100)\n",
    "plot_loss(loss_list_simple_net, 'Simple Net')\n",
    "plot_loss(loss_list_dense, 'ResNet')\n",
    "\n",
    "if right_edge_displacement==0.05:\n",
    "    ax.plot(100*np.arange(len(loss_list_dense)), 0.006*np.ones(len(loss_list_dense)), label='FEM Energy')\n",
    "elif right_edge_displacement==0.5:\n",
    "    ax.plot(100*np.arange(len(loss_list_dense)), 0.43*np.ones(len(loss_list_dense)), label='FEM Energy')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the body post-training\n",
    "plot_displacement(dense_net1, dense_net2, right_edge_displacement, scale_network_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using ResNet architecture seems to have improved the convergence as compared to the case of the simple network in part A. \n",
    "\n",
    "One reason for this is as we make the network deeper, it can hurt the ability to train the network to do well on the training set. ResNet overcomes this issue by introducing skip connections between layers; this helps better backpropagation of loss and better convergence.\n",
    "\n",
    "Can we further improve this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C: Fourier Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part let's not only use a ResNet architecture for our neural networks but also introduce a Fourier feature mapping for the input data. \n",
    "It has been observed that deeper networks are biased towards low frequency functions ([\"Over-parameterized networks prioritize learning simple patterns that generalize\n",
    "across data samples\"](http://proceedings.mlr.press/v97/rahaman19a/rahaman19a.pdf)). \n",
    "\n",
    "To solve this bias towards the low frequency function, [Tancik et al.](https://arxiv.org/pdf/2006.10739.pdf) show that passing the neural network inputs through a simple Fourier feature mapping not only helps neural network learn high-frequency function, but also, helps achieve faster convergence for high frequency components.  \n",
    "\n",
    "The create a neural network with Fourier feature, just set the parameter `fourier_features` as `True` in the `DenseResNet` class I defined above:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fourier_dense_net1 = DenseResNet(dim_in=2, dim_out=1, num_resnet_blocks=3, \n",
    "                        num_layers_per_block=2, num_neurons=50, activation=nn.Sigmoid(), \n",
    "                        fourier_features=True, m_freqs=100, sigma=10, tune_beta=False)\n",
    "\n",
    "Fourier_dense_net2 = DenseResNet(dim_in=2, dim_out=1, num_resnet_blocks=3, \n",
    "                        num_layers_per_block=2, num_neurons=50, activation=nn.Sigmoid(), \n",
    "                        fourier_features=True, m_freqs=100, sigma=10, tune_beta=False)\n",
    "\n",
    "# Here is the model capacity:\n",
    "model_capacity(Fourier_dense_net1)\n",
    "model_capacity(Fourier_dense_net2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is just a sanity check that we start from a candidate solution that indeed satisfies the boundary conditions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the body pre-training and make sure the boundary conditions are satified\n",
    "plot_displacement(Fourier_dense_net1, Fourier_dense_net2, right_edge_displacement, scale_network_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the network for 5000 iterations and batch size of 10. Training should take around 6-8 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now, let's train the network\n",
    "loss_list_dense_fourier, Fourier_dense_net1, Fourier_dense_net2 = train_MultiFieldFracture_seperate_net(net1=Fourier_dense_net1, \n",
    "                                                                                                     net2=Fourier_dense_net2, \n",
    "                                                                                                     right_edge_displacement = right_edge_displacement, \n",
    "                                                                                                     batch_size_X = 10, \n",
    "                                                                                                     max_iter = 5000, \n",
    "                                                                                                     print_results_every = 100, scale_network_out=scale_network_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Analyze Training Loss and Material Displacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the training loss\n",
    "figure, ax = plt.subplots(dpi=100)\n",
    "plot_loss(loss_list_simple_net, 'Simple Net')\n",
    "plot_loss(loss_list_dense, 'ResNet')\n",
    "plot_loss(loss_list_dense_fourier, 'Fourier-ResNet')\n",
    "\n",
    "if right_edge_displacement==0.05:\n",
    "    ax.plot(100*np.arange(len(loss_list_dense)), 0.006*np.ones(len(loss_list_dense)), label='FEM Energy')\n",
    "elif right_edge_displacement==0.5:\n",
    "    ax.plot(100*np.arange(len(loss_list_dense)), 0.43*np.ones(len(loss_list_dense)), label='FEM Energy')\n",
    "\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the body post-training\n",
    "plot_displacement(Fourier_dense_net1, Fourier_dense_net2, right_edge_displacement, scale_network_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This look great! The loss seems to have converged to the FEM solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "+ For part A, rerun the code for 20,000 iterations. Does the simple network in part A converge? How does the prediction look?\n",
    "+ How does batch size affect the convergence? Try different batch sizes (20, 40,...) by changing `batch_size_X` in the `train_MultiFieldFracture_seperate_net` function. Which one works the best? \n",
    "+ Change the activation function in the `DenseResNet` class from `nn.Sigmoid()` to `nn.ReLU()` and rerun the code. How does the convergence change?\n",
    "+ In the `DenseResNet` class, read the docstring to see what `tune_beta=True` does. Set `tune_beta=True` and rerun the code. Do you observe better convergence?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
